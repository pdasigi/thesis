% So far we have evaluated our NLU pipelines by measuring the accuracy of the prediction component at any given task. The encoder component has not been explicitly evaluated.
% This is because we generally view NLU pipelines as end-to-end systems, and do not care about the quality of the encoding as long as it serves as good features for the prediction
% component. In this chapter, we take a different view and try to evaluate the encoder by investigating how well the encoded representations can be transferred across tasks. Transfer
% learning in NLU systems is an attractive line of investigation for various reasons:
% \begin{itemize}
%  \item Not all language understanding tasks come with lots of training data. For some tasks it may be very difficult to get high quality annotations from humans due to factors such as 
%  the task being subjective or requiring domain expertise. Being able to pre-train encoders on one task that has large amounts of training data available (like SNLI \cite{bowman:15} for textual entailment), and use them for similar
%  tasks like paraphrasing or question answering, can address this issue.
%  \item From a language understanding perspective, the transferability of the encoders gives us a clearer idea of what kind of features they are learning.
%  \item Due to the large number of parameters involved, deep neural networks are highly prone to overfitting to the task. Transfer learning can address this problem. 
% \end{itemize}
% 
% Transfer learning in neural networks has been well-studied for vision related problems. It has been shown \citep{zeiler2014visualizing} that object recognition models trained on
% ImageNet \citep{deng2009imagenet} can be transfered to other object recognition datasets when the final classifier is retrained to the new dataset. A systematic study of the generality
% of features learned at various layers in deep neural networks trained on natural images was done by \cite{yosinski2014transferable}. For language related problems, there do not exist similar studies.
% There are however multi-task learning efforts for NLP tasks.  A notable example is the work by \cite{collobert2011natural} that showed the benefit of multi-task learning for surface-level NLP tasks 
% like Named Entity Recognition, Part of Speech Tagging and Semantic Role Labeling. In this chapter, we investigate the transferability of the encoder in an NLU system trained on one task, to other similar tasks.
% 
% \section{Proposed Work}
% The proposed work in this area involves answering the following concrete questions.
% \begin{enumerate}
%  \item Can different NLU tasks be jointly learned using the same neural network architecture?
%  \item In a scenario where the NLU system is pretrained on a task with large amounts of data, how does the complexity (in terms of number of parameters)
%  of the encoder affect its transferability to other tasks?
%  \item How do external knowledge inputs affect the transferability of the encoder?
% \end{enumerate}


\section{Summary}
In this thesis we propose to augment deep learning systems for NLU with external knowledge inputs. We identify two broad kinds of knowledge: \textit{background} and \textit{contextual}.

We defined background knowledge as the implicit shared human knowledge that is often omitted in human generated text or speech. Our proposal to incorporate this kind of knowledge in NLU systems is by linking text to structured knowledge bases like
WordNet, Freebase and ConceptNet, and consequently modify the encoder to process this additional information. In Chapter~\ref{chapter:ontolstm}, we shouwed the advantages of linking words to WordNet subgraphs, and how it helps textual entailment and 
prepositional phrase attachment tasks. By giving the encoder, access to the hypernym paths of relevant concepts we showed that we can learn useful task-dependent generalizations using an attention mechanism. We propose to use similar disambiguation
techniques to learn better representations of nested proposition structures by linking the nodes of a TreeLSTM to subgraph structures from Freebase, and better event representations by linking semantic role labeled structures with commonsense information from ConceptNet.

We defined contextual knowledge as the explicit additional information that reading comprehension systems need to reason over, during prediction. We identified that many reading comprehension tasks require modeling complex entailment,
which is beyond the capability of currently published memory networks. Our proposal in Chapter~\ref{chapter:memnet_qa} includes modifying the prediction component of memory networks to make the applicable to science question answering and modeling experiment narratives.

\section{Timeline}
\begin{itemize}
    \item Ontology Aware Recurrent Neural Networks: Done
    \begin{itemize}
        \item Target: ACL 2017
    \end{itemize}
    \item Complex entailment in memory networks for Science QA: Nov 2016 -- Feb 2017
    \begin{itemize}
        \item Target: ACL 2017
    \end{itemize}
    \item KB TreeLSTM for question answering: Feb 2017 -- May 2017
    \begin{itemize}
        \item Target: NIPS 2017 or EMNLP 2017
    \end{itemize}
    \item Improved event understanding with ConceptNet: May 2017 -- August 2017
    \item Transfer Learning with background knowledge: August 2017 -- November 2017
    \begin{itemize}
        \item Target: ICLR 2018
    \end{itemize}
    \item Thesis writing and defense: Spring 2018
\end{itemize}

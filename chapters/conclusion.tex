% So far we have evaluated our NLU pipelines by measuring the accuracy of the prediction component at any given task. The encoder component has not been explicitly evaluated.
% This is because we generally view NLU pipelines as end-to-end systems, and do not care about the quality of the encoding as long as it serves as good features for the prediction
% component. In this chapter, we take a different view and try to evaluate the encoder by investigating how well the encoded representations can be transferred across tasks. Transfer
% learning in NLU systems is an attractive line of investigation for various reasons:
% \begin{itemize}
%  \item Not all language understanding tasks come with lots of training data. For some tasks it may be very difficult to get high quality annotations from humans due to factors such as 
%  the task being subjective or requiring domain expertise. Being able to pre-train encoders on one task that has large amounts of training data available (like SNLI \cite{bowman:15} for textual entailment), and use them for similar
%  tasks like paraphrasing or question answering, can address this issue.
%  \item From a language understanding perspective, the transferability of the encoders gives us a clearer idea of what kind of features they are learning.
%  \item Due to the large number of parameters involved, deep neural networks are highly prone to overfitting to the task. Transfer learning can address this problem. 
% \end{itemize}
% 
% Transfer learning in neural networks has been well-studied for vision related problems. It has been shown \citep{zeiler2014visualizing} that object recognition models trained on
% ImageNet \citep{deng2009imagenet} can be transfered to other object recognition datasets when the final classifier is retrained to the new dataset. A systematic study of the generality
% of features learned at various layers in deep neural networks trained on natural images was done by \cite{yosinski2014transferable}. For language related problems, there do not exist similar studies.
% There are however multi-task learning efforts for NLP tasks.  A notable example is the work by \cite{collobert2011natural} that showed the benefit of multi-task learning for surface-level NLP tasks 
% like Named Entity Recognition, Part of Speech Tagging and Semantic Role Labeling. In this chapter, we investigate the transferability of the encoder in an NLU system trained on one task, to other similar tasks.
% 
% \section{Proposed Work}
% The proposed work in this area involves answering the following concrete questions.
% \begin{enumerate}
%  \item Can different NLU tasks be jointly learned using the same neural network architecture?
%  \item In a scenario where the NLU system is pretrained on a task with large amounts of data, how does the complexity (in terms of number of parameters)
%  of the encoder affect its transferability to other tasks?
%  \item How do external knowledge inputs affect the transferability of the encoder?
% \end{enumerate}


\section{Summary}
In this thesis we identified two classes of knowledge relevant to language understanding: \textit{background} and \textit{contextual}.

We defined background knowledge as the implicit shared human knowledge that is often omitted in human generated text or speech. We presented two ways to incorporate this kind of knowledge in NLU systems. In Chapter~\ref{chapter:nem}, we showed how selectional preferences
in structures relevant to the task at hand can be leveraged to encode background knowledge. We showed that this is indeed more effective than encoding whole sentences using comparable neural network architectures.
In the Chapter~\ref{chapter:ontolstm}, we linked text to a structured knowledge bases (WordNet) by building an ontology-aware encoder that exploited the WordNet hierarchies to learn context-sensitive token representations of words. We showed the proposed model helps textual entailment and 
prepositional phrase attachment tasks. By giving the encoder, access to the hypernym paths of relevant concepts we showed that we can learn useful task-dependent generalizations using an attention mechanism.

We defined contextual knowledge as the explicit additional information that reading comprehension systems need to reason over, during prediction. In this thesis, we proposed to pursue two research goals towards effectively reasoning over semi-structured and unstructured contextual knowledge.
First, as described in Chapter~\ref{chapter:nnsp}, we will explore type-driven neural network based semantic parsers to reason over semi-structured contexts for QA tasks where only the question-answer pairs are given. Our preliminary experiments with such a parser show promising
results on the \textsc{WikitablesQuestions} dataset. Second, we propose to augment explicit memory based neural network models with a stochastic retrieval module in Chapter~\ref{chapter:memnet_qa}. While there has been a lot of progress in explicit memory based models for reading comprehension, to the best of our knowledge, all the
work deals with problems where the context is finite and well defined. We will look at tasks where the relevant context needs to be retrieved before any reasoning can be performed over it, and our proposed approach involves jointly training the retrieval and reasoning components, and determining
when to stop retrieving contexts depending on the inputs.

\section{Timeline}
\begin{itemize}
    \item Leveraging Selectional Preferences to Model Events: Done
    \begin{itemize}
     \item Published at COLING 2014 (joint work with Ed Hovy)
    \end{itemize}

    \item Ontology Aware Token Embeddings: Done
    \begin{itemize}
        \item Accepted at ACL 2017 (joint work with Waleed Ammar, Chris Dyer, and Ed Hovy)
    \end{itemize}
    
    \item Type-Driven Neural Semantic Parsing: Partially done
    \begin{itemize}
     \item Initial Accepted at EMNLP 2017 (joint work with Jayant Krishnamurthy and Matt Gardner)
     \item Target: Improved model as a ICLR 2018 or TACL submission
    \end{itemize}

    \item Retrieval for Memory Networks: Not started
    \begin{itemize}
     \item Target: ACL 2018
    \end{itemize}
    
    \item Thesis writing and defense: End of Spring 2018
\end{itemize}

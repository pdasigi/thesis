Natural Language Understanding (NLU) systems process human generated text (or speech) at a deep semantic level and encode the meaning
of the processed inputs. Several concrete tasks have been defined within the field on Computational Linguistics (CL) to evaluate the encoded meaning representations.
In the case of Semantic Parsing, the output is a logical form that can be queried against a knowledge base of facts to compute its truth value. Textual Entailment refers to the 
problem of identifying whether the truth of some text provided as a hypothesis follows from that of another text provided as a premise. Sentiment Analysis is the process of 
automatically categorizing the opinions expressed in the inputs towards specific targets. All these tasks require encoding the semantics of the input in a way that is
at least good enough to perform well at the given task, with the inherent assumption that an improvement in the task performance correlates with an improvement in the
quality of the semantic representation or in other words, the understanding capability of the computational system.
% TODO: Improve the following statement and uncomment it.
%Each task defines a restricted set of linguistic aspects that can be taught to a computational system, and are easy enough to automatically evaluate
%on unseen examples, but are complex enough for the representation techniques to be applicable to understanding the unrestricted set.

A generic NLU system can thus be succinctly described using the following two equations.
\begin{align}
 \mathbf{e} &= \mathtt{encode}(\mathbf{I}) \label{eq:generic_encoding}\\
 \mathbf{o} &= \mathtt{predict}(\mathbf{e}) \label{eq:generic_prediction}
\end{align}
where $\mathbf{I}$ is the set of textual inputs to the NLU system. For example, $\mathbf{I}$  are single sentences in Sentiment Analysis and pairs of sentences in Textual Entailment. $\textbf{e}$ are intermediate 
encoded semantic representations of the inputs (which may or may not be task specific), and $\mathbf{o}$ are the final task specific predictions. For example, in Sentiment Analysis or Textual Entailment, 
$\mathbf{o}$ are categorical labels indicating the sentiment or entailment respectively, and in the case of Semantic Parsing, they are structured outputs or parses. In older feature-rich methods for NLU, 
Equation~\ref{eq:generic_encoding} is a mapping of the inputs to a hand designed feature space, typically containing patterns over n-grams or shallow linguistic features like part-of-speech tags. In such systems,
the modeling emphasis was more on the prediction component, and the encoding component did not involve any learning. More recent systems use representation learning techniques to also learn the parameters of the
$\mathtt{encode}$ function, and typically this is done jointly with learning the parameters of the $\mathtt{predict}$ function.

\section{External Knowledge}
An issue with the formulation of the problem in Equation~\ref{eq:generic_encoding} and Equation~\ref{eq:generic_prediction} is that they are missing external knowledge. 
Since human language is aimed at other humans who share 
the same background knowledge, a lot of information is often not explicitly stated
for the sake of brevity. The implicit knowledge required to understand language varies 
from simple commonsense in the case of basic conversations, to complex principles that link 
concepts in more esoteric communications. Consider an example of the former in solving the 
textual entailment problem given the following premise, and a candidate hypothesis:
\begin{itemize}
 \item \textbf{Premise:} \textit{Children and parents are splashing water at the pool.}
 \item \textbf{Hypothesis} \textit{Families are playing outside.}
\end{itemize}
The knowledge a human uses to correctly predict that the hypothesis can be inferred from the 
premise is that \textit{children and parents} typically form \textit{families}, \textit{splashing water} 
is a kind of \textit{playing}, and a \textit{pool} is expected to be \textit{outside}. Similarly, an 
automated system that reads a science text book to answer the following 
question requires the knowledge of properties of materials.
\begin{itemize}
 \item \textit{Which of the following is the best conductor of electricity?}\\ 
  A. \textit{glass rod}  B. \textit{wooden stick}  C. \textit{plastic straw} D. \textit{metal nail}
\end{itemize}
Clearly, we need some external knowledge as an input to our NLU systems to perform well at tasks like this.
Without this additional context, the NLU systems will merely memorize specific patterns seen in the training data
and cannot generalize to unseen test cases. Accordingly, we modify our NLU equations as follows.
\begin{align}
 \mathbf{e} &= \mathtt{encode}(\mathbf{I}, \mathbf{K}_e) \label{eq:encoding_with_knowledge}\\
 \mathbf{o} &= \mathtt{predict}(\mathbf{e}, \mathbf{K}_p) \label{eq:prediction_with_knowledge}
\end{align}
where $\mathbf{K}_e$ and $\mathbf{K}_p$ represent the knowledge required for encoding and prediction respectively. 
They serve different purposes. While $\mathbf{K}_e$ is additional knowledge used to better compose the units needed to
encode input text, $\mathbf{K}_p$ helps with reasoning. Two examples of $\mathbf{K}_e$ are hypernym trees from WordNet to
incorporate commonsense information about concepts, and subgraph features from Freebase to encode relations
between entities seen in the input text. $\textbf{K}_p$ inputs might come from 

The additional knowledge humans use to do deep semantic processing is encoded, to
some extent in knowledge bases (KB) and ontologies. However, using that information in NLU systems is a 
hard task. Firstly, while linking the words being read to the structured knowledge
in a KB, an automated system faces ambiguity. For example, with lexical ontologies like WordNet, 
we get useful type hierarchies like \textit{parent is-a ancestor is-a person} and \textit{pool is-a body-of-water} 
and so on, but one has to deal with sense ambiguity: \textit{pool} can also be a game. Moreover, finding the 
relevant parts of the KB given the text being processed is a challenge. For example, assuming we have a KB that encodes
the properties of materials, to answer the question in the second example above, we still have to 
find the conducting properties, and possibly generalize the properties of various specific metals to
infer that metals are generally good conductors of electricity.

The goal of this thesis is to find the limitations of distributional and the ontological 
sources of information and use this knowledge to build hybrid NLU systems that can successfully 
incorporate structured or unstructured background knowledge in deep learning models. The fact
that the two sources of semantic information are fundamentally different 
makes this challenging. While distributional approaches encode meaning in a
continuous and an abstract fashion, meaning in KBs is symbolic and discrete. 
A major chunk of this thesis will be dedicated to methods of incorporating
symbolic knowledge of various kinds in neural networks, and learning 
distributions over the discrete concepts of the KB conditioned on the context,
to deal with exceptions in language.

\section{Generic NLU Pipeline}

% TODO: Show a generic deep learning pipeline for NLU and talk about which parts can be improved and how.

\section{Outline}
In the first part of this thesis, I will explore the limits of purely distributional models
and study the kinds of information that cannot be modeled by them. A candidate problem for this
exercise is semantic anomaly detection, which involves automatically 
identifying real world commonsense violations in text. I shall describe an annotation
effort that resulted in newswire headlines manually labeled with the degree
of surprise associated with them. The task is to build a model that can identify the highly
surprising headlines as anomalies. This is different from the usual Language Modeling (LM) problem
because even the semantic anomalies are well-formed sentences that can be well-understood. Consequently,
the model needs to discriminate between sentences at a semantic level deeper than surface fluency.
I shall explore to what extent neural network language models (NNLM), the current state of the art in LM,
are applicable in semantic anomaly detection. A popular technique used to avoid normalizing the probabilities
over the entire vocabulary in NNLM is Noise Contrastive Estimation (NCE), which modifies the problem of 
building a generative model into that of building a discriminative one to distinguish the data distribution
from a predefined noise distribution. A challenge in building NNLM for semantic anomaly detection is that defining 
the noise distribution in this case is not trivial. One way to address this challenge is to use ideas from 
adversarial networks, where the noise samples themselves are generated by a neural network whose complexity
is comparable to that of the network that discriminates the noise from the data. A baseline model for this 
problem is a previously published supervised Recursive Neural Network trained for the same problem.


Next, I will describe a method to incorporate selectional 
preferences in Machine Reading. This involves a variant of Long Short-Term
Memory (LSTM) based Recurrent Neural Networks (RNN) that use information
from WordNet, a lexical 
ontology. The hybrid model looks at WordNet synsets, and the hypernym hierarchies of the
words being processed so that the 
LSTM is aware of their different senses, and the corresponding type information.
The ontology aware LSTM (OntoLSTM) learns to attend to the appropriate sense,
and the relevant type 
(either the most specific concept or a generalization by choosing a hypernym of
the word), conditioned on the context and the end-objective. I show that when
trained in an 
unsupervised setting as a Language Model (LM), OntoLSTM has a lower perplexity
than traditional LSTMs, and also does well at unsupervised Word Sense
Disambiguation (WSD). In a supervised setting, 
it outperforms traditional LSTMs in predicting textual entailment.

The third part of the thesis contains extensions of the idea to model factual knowledge, towards
systems that answer non-factoid questions, particularly in general science. Such systems 
need to reason about general principles behind facts, and thus distributional information
alone is not enough. I will show methods that encode graph based features from KBs relevant to the 
question, as high dimensional vector representations such that they can provide 
additional context. Whereas the structured information used in the first part has only one kind of
relation, the KB used in this part has several relation types. 

The previous two parts of the thesis deal with words or multi-word expressions as units being grounded in KBs. 
There are other kinds of frame structures that these techniques can be extended to. One example is the rhetorical 
structure in discourse. The units of operation in this case are clauses instead of words. This is one potential 
direction for the third part of the thesis. Another direction is automatic construction of lexical ontologies in 
low resource languages. Applying the techniques described in the second part, we may be able to induce senses and 
hierarchies of concepts in a new language using only distributional information.
\section{Generic View of Memory Networks}
Memory Networks (MemNN) \citep{weston2014memory} are a class of learning models
that combine inference with long-term memory. Unlike Recurrent Neural Networks
(RNN) that model language \citep{mikolov2010recurrent}, and their variants with
Long Short-Term Memory (LSTM) \citep{hochreiter1997long}, MemNNs store a global
memory with read and write functions. While MemNN required explicit supervision
for selecting the relevant parts of the memory, \citep{sukhbaatar2015end}
proposed a end-to-end variant (MemN2N) where the memory selection component is
trained jointly with the rest of the network. These were previously used for
answering questions that require reasoning over multiple background sentences,
both in simulated \citep{bordes2010towards} and large-scale
\citep{fader2013paraphrase} scenarios.
\begin{figure*}
\begin{center}
  \includegraphics[width=6.5in]{figures/memory_network_generic.png}
  \caption{Schematic showing a generic view of end-to-end memory network}
  \label{fig:memnet}
  \end{center}
\end{figure*}
Figure~\ref{fig:memnet} shows the setup of our memory network model, which is a
generalization of the MemN2N model \citep{sukhbaatar2015end}. Our model takes as
input a set of $N$ background sentences indexed as $\{b_i\}_{i=1}^N$, such that
$b_i$ is a vector containing the indices of words in the $i^\text{th}$ sentence.
The sentences are then encoded using the \texttt{EncodeBackground} function to
produce the matrix $B \in \mathbb{R}^{n \times d}$, where each row $B[i]$ is the
encoding of sentence $b_i$. In addition, the model also takes as input a query
indexed as $q$, which is a vector containing the indices of the words in the
query similar to $b_i$ vectors. The query is encoded using \texttt{EncodeQuery}
to produce $u^0 \in \mathbb{R}^d$. The memory network can have multiple memory
layers, corresponding to multiple hops. At each hop, a memory layer receives as
input the output from the previous hop $u^{h-1}$, which is passed to the
\texttt{SelectMemory} function, which uses an attention mechanism to select the
relevant parts of the encoded background, conditioned on $u^{h-1}$ and produces
a summary $s^h$, of the background encoding for the current hop. $s^h$ is then
passed to \texttt{UpdateMemory} along with $u^{h-1}$, to produce the updated
memory representation $u^h$ for the current hop. It has to be noted that the
initial $u^0$ is the encoding of the query itself. Finally, an answer is
predicted by passing the query encoding $u^0$, and the summary of the
background, $s^H$ from the final hop $H$ to the \texttt{PredictAnswer} function.

It can be seen that MemN2N fits into this setup with the following
configuration:
% TODO(pradeep): MemN2N actually has two Embedding matrices encoding background
% for input and output. Also, there is another variant which encodes positions of
% the words too.
\begin{flalign*}
&\texttt{EncodeQuery}(q) = \text{Embedding}_q(q) \\
&\texttt{EncodeBackground}(b_i) = \text{Embedding}_b(b_i) \\
&\texttt{SelectMemory}(u^{h-1}, B) = \text{softmax}(B.u^{h-1}).B \\
&\texttt{UpdateMemory}(u^{h-1}, s^h) = u^{h-1} + s^h \\
&\texttt{PredictAnswer}(u_0, s^H) = \text{softmax}(W.s^H)
\end{flalign*}
where $\text{Embedding}_q(.)$ and $\text{Embedding}_b(.)$ are simply bag of
words models that aggregate the vector representations of all the words given by
the indices in the input. $W \in \mathbb{R}^{V \times d}$ is a parameter of the
answer prediction function, causing the softmax to be over the vocabulary size
$V$. Note that in MemN2N, $u^0$ is not an argument of \texttt{PredictAnswer}.

In this chapter, we propose to apply memory networks to two different tasks:
answering science questions and understanding experiment narratives in research papers. 

\section{Memory Networks for Science Question Answering}
The ScienceQA dataset we use here is significantly different from the
QA datasets previously used to test memory networks, and requires more complex
reasoning. One example of such question is shown below.
%TODO(pradeep): Make this a table?
\begin{itemize}
\item Astronauts weigh more on Earth than they do on the moon because
\begin{enumerate}[(a)]
 \item they have less mass on the moon
 \item their density decreases on the moon
 \item the moon has less gravity than Earth
 \item the moon has less friction than Earth
\end{enumerate}
\end{itemize}
%TODO(pradeep): Say more things about the nature of the problem.
In this chapter, we take a generic view of MemN2N as shown in
Figure~\ref{fig:memnet} and identify five configurable components in it:
\textit{query encoder}, \textit{background encoder}, \textit{memory selector},
\textit{memory updater} and an \textit{answer prediction} module, all
implementing appropriate functions. We experiment with various configurations of
these components and show results on the science QA dataset. It has to be noted
that given good representations of the question, candidate answers and
background information, QA can be seen as a textual entailment problem.
\citep{bowman2015large}.

\paragraph{Science Question Answering as Textual Entailment} In our setup, we
transform the problem of answering science questions into a textual entailment
problem. That is, given a multiple choice question with answer options, we
convert the combination of the question and each of the options into a
statement, and check whether the statement can be entailed from relevant
background information. Given this setup, the \texttt{PredictAnswer} function
essentially becomes an entailment function.

\subsection{Preliminary Results}
We now show some preliminary results of our memory network implementation on science question answering using MemN2N architecture with following change in configuration:
\begin{flalign*}
&\texttt{PredictAnswer}(u_0, s^H) = \texttt{HeuristicMatch}(u_0, s_h)
\end{flalign*}
where \texttt{HeuristicMatch}(.) is the heuristic matching function proposed by \cite{mou2015recognizing} for textual entailment, also used for our experiments with SNLI data in 
Chapter~\ref{chapter:ontolstm}. The dataset used for these experiments is questions collected from 4th and 8th grade science text books.
Context for each question was obtained as follows. We built a Lucene index over a big collection of sentences related to general science from various sources and extracted relevant background sentences for each question by querying it. For each of the
options, we query the Lucene index with a combination of the option text and the question. We thus transform questions like the one shown above into four entailment problems where the combination of the
question and one of the options is the hypothesis and the relevant background sentences are the premises. The final answer is picked by selecting the option (combined with the question) that has the highest
entailment score. This is done by passing the final entailment scores through a softmax layer.

Preliminary results using our model are shown in Table~\ref{tab:memnet_qa_results}, using two different encoders
to encode the input sentences and background. We also show the accuracy of a baseline system based on Lucene, that selects for every question, the option that results in the highest relevance score in the process
described above for retrieving background sentences. It can be seen that the simple baseline does better than the memory network model.

\begin{table}
    \centering
    \begin{tabular}{|l|c|}
    \hline
    \textbf{Encoder} & \textbf{Test Acc.}\\
    \hline
    BOW & \% \\
    GRU & 38.8\% \\
    \hline
    \hline
    \textbf{Lucene baseline} & 41.1\% \\
    \hline
    \end{tabular}
    \caption{Results of our Memory Network on ScienceQA in comparison with a Lucene baseline}
    \label{tab:memnet_qa_results}
\end{table}

\subsection{Analysis}

\subsection{Proposed Work}


\section{Memory Networks for Understanding Experiments}
\subsection{Discourse Structure of Experiments}
An important part of science is communicating results. There are well established rhetorical guidelines 
\cite{michael1996craft} for scientific writing that are used across disciplines and consequently, narratives describing
evidence within a scientific investigation are expected to have a certain structure. Typically, the description begins with certain background 
information which has already been proved, 
followed by some motivating hypotheses to introduce the experiment, the methods, followed by results, and finally
inferences made based on those results. Understanding this structure is important since it enables the 
higher-level construction of the general argument of the paper. The reader assembles the pieces in order to
understand what was done, why it was done, what prior knowledge it builds upon and/or refutes, and with what 
certainty the final conclusions should be accepted. Without such an overall model of the experiment, 
the reader has nothing but basic assertions. Identifying the structure of experiments provides
additional context information that can help various downstream tasks. For event
co-reference, 
one can use the structure to accurately resolve anaphora links. For example, a
reference made in an \textit{implication} statement is likely to some entity in
a \textit{result} 
statement it follows. It has to be noted that the taxonomy also provides
epistemic information which is helpful in information extraction (IE): IE
systems need not process
clauses labeled as \textit{hypothesis} or \textit{goal} since they do not
contain events that actually occurred.
\begin{table}
\begin{center}
  \begin{tabular}[c]{|c|l|}
 \hline
  \textbf{Type} & \textbf{Definition} \\
  \hline
  Goal & Research goal \\
  \hline
  Fact & A known fact, a statement taken to be true by the author \\
  \hline
  Result & The outcome of an experiment \\
  \hline
  Hypothesis & A claim proposed by the author \\
  \hline
  Method & Experimental method \\
  \hline
  Problem & An unresolved or contradictory issue \\
  \hline
  Implication & An interpretation of the results \\
  \hline
  \end{tabular}
\end{center}
 \caption{Seven label experiment discourse taxonomy from \protect\cite{de2012verb}}
 \label{table:taxonomy}
\end{table}

In this work, our aim is to identify these
discourse elements given an experiment narrative. There exist several proposals for experiment 
discourse models \cite{liakata2010zones,nawaz2010evaluating,mizuta2004annotation,nwogu1997medical}. 
We adopt the discourse type
taxonomy 
for biological papers suggested by \cite{de2012verb}, and define our problem
as identifying the discourse type of each clause in a given experiment
description.
The taxonomy contains seven types shown in Table~\ref{table:taxonomy}.
Figure~\ref{fig:example_parse} shows an example of a paragraph from a
paper\footnote{\textit{Protein tyrosine phosphatase-PEST 
regulates focal adhesion disassembly, migration, and cytokinesis in
fibroblasts}. Angers-Loustau et al., The Journal of Cell Biology, 1999} broken
down into clauses and
tagged with discourse types.
\begin{figure}
  \begin{center}
  \includegraphics[width=5in]{figures/example_text.png}
  \caption{Example of an experiment description tagged with discourse types.}
  \label{fig:example_parse}
  \end{center}
 \end{figure}

\subsection{Data}
\subsection{Proposed Work}
%for a more compact document, add the option openany to avoid
%starting all chapters on odd numbered pages
\documentclass[12pt]{cmuthesis}

% This is a template for a CMU thesis.  It is 18 pages without any content :-)
% The source for this is pulled from a variety of sources and people.
% Here's a partial list of people who may or may have not contributed:
%
%        bnoble   = Brian Noble
%        caruana  = Rich Caruana
%        colohan  = Chris Colohan
%        jab      = Justin Boyan
%        josullvn = Joseph O'Sullivan
%        jrs      = Jonathan Shewchuk
%        kosak    = Corey Kosak
%        mjz      = Matt Zekauskas (mattz@cs)
%        pdinda   = Peter Dinda
%        pfr      = Patrick Riley
%        dkoes = David Koes (me)

% My main contribution is putting everything into a single class files and small
% template since I prefer this to some complicated sprawling directory tree with
% makefiles.

% some useful packages
\usepackage{times}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{algorithm2e}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage[numbers,sort]{natbib}
\usepackage[backref,pageanchor=true,plainpages=false, pdfpagelabels, bookmarks,bookmarksnumbered,
%pdfborder=0 0 0,  %removes outlines around hyper links in online display
]{hyperref}
\usepackage{subfigure}

\DeclareMathOperator*{\argmin}{arg\,min}

% Approximately 1" margins, more space on binding side
%\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar]{geometry}
%for general printing (not binding)
\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

% Provides a draft mark at the top of the document. 
\draftstamp{\today}{DRAFT}

\begin {document} 
\frontmatter

%initialize page style, so contents come out right (see bot) -mjz
\pagestyle{empty}

\title{ {\it \huge Thesis Proposal}\\
{\bf Knowledge-aware Natural Language Understanding}}
\author{Pradeep Dasigi}
\date{}
\Year{2017}
\trnumber{}

\committee{
Eduard Hovy (Chair) \\
Chris Dyer \\
William Cohen \\
Luke Zettlemoyer
}

\support{}
\disclaimer{}

% copyright notice generated automatically from Year and author.
% permission added if \permission{} given.

\keywords{natural language understanding, knowledge, neural networks, end-to-end models, semantic parsing, question answering}

\maketitle

% TODO: Uncomment this for the dissertation
% \begin{dedication}
% 
% \end{dedication}

\pagestyle{plain} % for toc, was empty

%% Obviously, it's probably a good idea to break the various sections of your thesis
%% into different files and input them into this file...

\begin{abstract}
Natural Language Understanding (NLU) systems
process human generated text (or speech) at a deep semantic level in a way that is useful for downstream applications such as
question answering, textual entailment or summarization. In this thesis, we examine the role of 
knowledge in NLU, and build systems that effectively encode and reason over knowledge.
We identify two kinds of knowledge, and make the following distinction between them: \textbf{implicit knowledge} is that which
is missing in the input utterances, but necessary for understanding them, and \textbf{explicit knowledge} is the additional context that
NLU systems need to reason over, for complete comprehension. A generic NLU pipeline typically involves two main components. 
The first is \textbf{encoding}, where words (or other basic linguistic units) within the inputs are composed to compute encoded representations, 
which are then used as features in the second component, \textbf{prediction}, to produce the desired output. We describe several NLU systems
that follow this pipeline, but with encoders that also encode implicit knowledge, or predictors that effectively reason over explicit knowledge.

The first part of the thesis deals with implicit or background knowledge. While distributional methods for encoding inputs have been
successfully used to represent meaning of words in the context of other words in the input,
and thus learn complex functions of their distributional properties, there are other aspects of semantics that are out of their reach. These aspects are related to
commonsense or real world information which is part of shared human knowledge but is not explicitly present in the input. We address this limitation
using two methods that rely on external resources. The first is by modeling the selectional restrictions verbs place on their
semantic role fillers. We use this model to encode events, and show that these event representations are useful in detecting newswire anomalies.
Our second approach towards augmenting distributional methods is using external knowledge bases like WordNet. We compute
ontology-grounded token-level representations of words and show that these representations are useful in predicting prepositional phrase attachments and textual entailment.

The second part of the thesis focuses on reasoning over explicit knowledge. Many machine comprehension tasks require interpreting input utterances in context of other explicit structured
or unstructured information. For structured contexts, we first describe a type constrained neural semantic parsing framework for answering questions
given semi-structured tables as the context. With simple entity linking and context-graph embedding methods, we achieve state of the art performance on the \textsc{WikiTableQuestions} dataset.
Proposed work in this area includes improved entity linking, and application of this framework to answering questions in other domains. For unstructured contexts, we propose to investigate
the role of retrieval methods for obtaining contexts in question answering using neural network models with explicit memory components,
and build models that can adaptively reason and learn to retrieve relevant context given a question.

\end{abstract}

% TODO: Uncomment this for the dissertation
% \begin{acknowledgments}
% 
% \end{acknowledgments}



\tableofcontents
\listoffigures
\listoftables

\mainmatter

%% Double space document for easy review:
%\renewcommand{\baselinestretch}{1.66}\normalsize

% The other requirements Catherine has:
%
%  - avoid large margins.  She wants the thesis to use fewer pages, 
%    especially if it requires colour printing.
%
%  - The thesis should be formatted for double-sided printing.  This
%    means that all chapters, acknowledgements, table of contents, etc.
%    should start on odd numbered (right facing) pages.
%
%  - You need to use the department standard tech report title page.  I
%    have tried to ensure that the title page here conforms to this
%    standard.
%
%  - Use a nice serif font, such as Times Roman.  Sans serif looks bad.
%
% Other than that, just make it look good...


\chapter{Introduction}
\label{chapter:introduction}
\input{chapters/introduction}
\part{Encoding Implicit Knowledge}
\chapter{Leveraging Selectional Preferences for Event Understanding}
\label{chapter:nem}
\input{chapters/modeling_events}
\chapter{Sentence Understanding with Background Knowledge from Ontologies}
\label{chapter:ontolstm}
\input{chapters/ontolstm}
\part{Reasoning over Explicit Knowledge}
\chapter{Semantic Parsing over Semi-Structured Tables}
\label{chapter:neural_semantic_parsing}
\input{chapters/neural_semantic_parsing}
\chapter{Proposed Work: Context Retrieval for Memory Networks}
\label{chapter:memnet_qa}
\input{chapters/memnet_qa}
\chapter{Summary and Timeline}
\input{chapters/conclusion}

%\appendix
%\include{appendix}

\backmatter

%\renewcommand{\baselinestretch}{1.0}\normalsize

% By default \bibsection is \chapter*, but we really want this to show
% up in the table of contents and pdf bookmarks.
\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
\bibliographystyle{plainnat}
\bibliography{thesis} %your bib file

\end{document}

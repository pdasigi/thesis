%for a more compact document, add the option openany to avoid
%starting all chapters on odd numbered pages
\documentclass[12pt]{cmuthesis}

% This is a template for a CMU thesis.  It is 18 pages without any content :-)
% The source for this is pulled from a variety of sources and people.
% Here's a partial list of people who may or may have not contributed:
%
%        bnoble   = Brian Noble
%        caruana  = Rich Caruana
%        colohan  = Chris Colohan
%        jab      = Justin Boyan
%        josullvn = Joseph O'Sullivan
%        jrs      = Jonathan Shewchuk
%        kosak    = Corey Kosak
%        mjz      = Matt Zekauskas (mattz@cs)
%        pdinda   = Peter Dinda
%        pfr      = Patrick Riley
%        dkoes = David Koes (me)

% My main contribution is putting everything into a single class files and small
% template since I prefer this to some complicated sprawling directory tree with
% makefiles.

% some useful packages
\usepackage{times}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{multirow}
\usepackage{enumerate}
\usepackage[numbers,sort]{natbib}
\usepackage[backref,pageanchor=true,plainpages=false, pdfpagelabels, bookmarks,bookmarksnumbered,
%pdfborder=0 0 0,  %removes outlines around hyper links in online display
]{hyperref}
\usepackage{subfigure}

\DeclareMathOperator*{\argmin}{arg\,min}

% Approximately 1" margins, more space on binding side
%\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar]{geometry}
%for general printing (not binding)
\usepackage[letterpaper,twoside,vscale=.8,hscale=.75,nomarginpar,hmarginratio=1:1]{geometry}

% Provides a draft mark at the top of the document. 
\draftstamp{\today}{DRAFT}

\begin {document} 
\frontmatter

%initialize page style, so contents come out right (see bot) -mjz
\pagestyle{empty}

\title{ {\it \huge Thesis Proposal}\\
{\bf Knowledge-aware Natural Language Understanding}}
\author{Pradeep Dasigi}
\date{}
\Year{2016}
\trnumber{}

\committee{
Eduard Hovy, CMU (Chair) \\
Chris Dyer, CMU, Google Deepmind \\
William Cohen, CMU \\
Sumit Chopra, Facebook AI Research
}

\support{}
\disclaimer{}

% copyright notice generated automatically from Year and author.
% permission added if \permission{} given.

\keywords{neural networks, knowledge base, hybrid NLU models}

\maketitle

% TODO: Uncomment this for the dissertation
% \begin{dedication}
% 
% \end{dedication}

\pagestyle{plain} % for toc, was empty

%% Obviously, it's probably a good idea to break the various sections of your thesis
%% into different files and input them into this file...

\begin{abstract}
Natural Language Understanding (NLU) systems
process human generated text (or speech) at a deep semantic level and organize
the information in a way that is useful for downstream applications such as
textual entailment, summarization and question answering. Recent
advances in NLU rely heavily on representation learning or deep learning approaches
that model text meaning in context. These approaches generally involve two main steps.
The first is encoding, where words (or other basic linguistic units) within the input
sentences are composed to get a unified representation per sentence. These encoded representations
are then used in the second step as features in a classifier or a structured prediction component to produce the desired
output. In this thesis, we identify two main weaknesses with this generic NLU pipeline, and propose address them by incorporating
external knowledge inputs.

Firstly, while distributional methods for 
encoding inputs have been successfully used to represent meaning of words in the context of other words,
and thus learn complex functions of their distributional properties, there are other
aspects of semantics that are out of their reach. These aspects are related to
commonsense or real world information which is part of shared human knowledge but is not explicitly
present in input. We address this issue using knowledge-aware encoders that produce representations grounded in
knowledge bases like WordNet and Freebase. 

Secondly, many NLU tasks require additional contextual information beyond a single sentence. Examples of such tasks
include machine comprehension, where the system is required to read a paragraph and answer questions about it. These tasks
require models that can perform inference over long-term memory. Recent progress in memory networks addresses this requirement
to some extent. However, the memory network models published so far have been shown to be capable of only shallow reasoning. We apply
these models to the task of answering non-factoid questions from science text books, and our initial results show that there is a lot of room
for improvement. We identify the difficulty of the the problem and propose some solutions to improve these models.

Finally, we propose to investigate the transferability of learned representations from one NLU task to another. Concretely, our experiments
will answer whether similar NLU tasks can be modeled jointly, whether decreasing the complexity of the encoders makes 
them overfit less to the task they are trained on and make them more transferable, and what role external knowledge plays in transfer learning.

\end{abstract}

% TODO: Uncomment this for the dissertation
% \begin{acknowledgments}
% 
% \end{acknowledgments}



\tableofcontents
\listoffigures
\listoftables

\mainmatter

%% Double space document for easy review:
%\renewcommand{\baselinestretch}{1.66}\normalsize

% The other requirements Catherine has:
%
%  - avoid large margins.  She wants the thesis to use fewer pages, 
%    especially if it requires colour printing.
%
%  - The thesis should be formatted for double-sided printing.  This
%    means that all chapters, acknowledgements, table of contents, etc.
%    should start on odd numbered (right facing) pages.
%
%  - You need to use the department standard tech report title page.  I
%    have tried to ensure that the title page here conforms to this
%    standard.
%
%  - Use a nice serif font, such as Times Roman.  Sans serif looks bad.
%
% Other than that, just make it look good...


\chapter{Introduction}
\label{chapter:introduction}
\input{chapters/introduction}
\chapter{Learning Representations Using Knowledge}
\label{chapter:ontolstm}
\input{chapters/ontolstm}
\chapter{Reasoning Using Knowledge: Memory Networks for Deep Reasoning}
\label{chapter:memnet_qa}
\input{chapters/memnet_qa}
\chapter{Non-standard NLU Tasks}
\label{chapter:other_tasks}
\input{chapters/other_tasks}
\chapter{Transfer Learning}
\label{chapter:transfer_learning}
\input{chapters/transfer_learning}
\chapter{Summary and Timeline}
\input{chapters/conclusion}

%\appendix
%\include{appendix}

\backmatter

%\renewcommand{\baselinestretch}{1.0}\normalsize

% By default \bibsection is \chapter*, but we really want this to show
% up in the table of contents and pdf bookmarks.
\renewcommand{\bibsection}{\chapter{\bibname}}
%\newcommand{\bibpreamble}{This text goes between the ``Bibliography''
%  header and the actual list of references}
\bibliographystyle{plainnat}
\bibliography{thesis} %your bib file

\end{document}
